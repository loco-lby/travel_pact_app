Hi, and welcome to Optimize Environments for visionOS.
 I’m Alex, a Technical Artist here at Apple.
 And in this video, we’re going to learn how to create immersive environments to make your stories feel real on Apple Vision Pro.
On this platform, you get to work with ultra-high resolution, fast frame rates, and seamless real-time interactions.
 This gives you a unique opportunity to build environments that deliver a true sense of presence.
 When you get it right, people feel transported.
 To achieve this, the goal is a cinematic level of visuals, helping tell stories like never before.
 Unfortunately, high-fidelity real-time rendering is expensive.
 Features such as soft shadows, global illumination, and complex shading, they all come at a high cost.
 So how do you bridge this gap? Well, it’s not just a single technique.
 It takes a flexible strategy, one that works within the technical limits, but still pushes visual quality as far as it can go.
 Let’s take a look at the optimization workflow we can use to make this happen.
 It starts with a pre-rendered image, capturing cinematic lighting, rich detail, and high-end materials.
 Next, geometry and textures are optimized based on what the viewer actually experiences.
 Then that pre-rendered image is transferred onto the optimized environment, baking in the look of a high-end render.
 Finally, everything is assembled in a real-time editor, like Reality Composer or Unity.
The result is visuals that feel truly immersive.
 In this video, I’m going to take you through two parts of this workflow: optimizing and baking your assets, to help you create stunning environments that you can use in your Vision Pro apps.
Now, you could do a lot of this manually, but it takes a while, certainly longer than we have in this video.
 Instead, I’m going to show you how you can use procedural tools to build a workflow that can automate most of this process.
 There are many ways to optimize 3D content, but one solid approach is using Houdini, a powerful application built around node-based workflows.
 Its visual scripting system lets you build procedural recipes to automate and calculate complex tasks.
 With Houdini, you can also use custom-built tools called Houdini Digital Assets, or HDAs.
 They blend visual scripting with underlying code and present key controls as editable parameters through a custom user interface.
 We’ve developed a versatile set of 14 powerful tools tools that you can download built to help optimize your artistic vision, while giving you precise controls every step of the way.
This workflow won’t cover every use case, but it gives you a solid foundation to take on the unique challenges of immersive content creation.
In this video, I’ll use the Moon environment as a case study.
 Walking through step-by-step how to prepare this content for real-time performance on Vision Pro.
 To those who are new to creating assets for Vision Pro, I’ll begin with a quick overview from a graphics perspective and what you need to know when building immersive content.
Then I’ll walk through the optimization process, starting with how to use the Immersive Boundary to optimize geometry beyond traditional methods.
 Next, how those same principles can be applied to textures through UV layout and texture baking techniques.
 With everything optimized, I’ll prepare this environment for USD, ready to bring into a real-time engine.
 Let's begin.
 As my colleague Scott mentioned in his WWDC24 video, “Optimize your 3D assets for spatial computing,” authoring content on Vision Pro is different.
 When creating content for this device, it’s crucial to understand how people will experience it, because this can shape your choices during the development process.
This is important when you consider how much rendering Vision Pro does for fully immersive scenes.
 In mixed immersion, only part of the display is rendered over passthrough video, but in full immersion, every pixel is rendered, making performance more challenging.
 When someone is fully immersed, they’re in a fixed, traversable space.
 They can physically move a few meters in any direction before the experience fades away and is replaced by reality.
 This is Vision Pro’s Immersive Boundary.
 This helps keep people safe in their surroundings, and it’s also an opportunity to maximize your graphics budget.
By understanding exactly where someone can move and look, you don’t need to render everything at full quality, just the parts that matter.
 No more, no less.
 This is the key to environment optimization, because high fidelity environments can often be very detailed, packed with polygons.
 Our pre-rendered content of the moon, it has over 100 million.
 That’s not going to run so well, but it gives the perfect opportunity to see just how far these Houdini tools can take us.
 Let’s get started by opening up our tools package, setting up our scene, and taking a tool of our optimization workflow.
Once you download the Houdini sample, go ahead and open the project folder.
 It contains everything needed to follow along.
 At the root is the main Houdini file, optimize.
hip.
 This holds the full setup we’ll be exploring.
 Right next to it is the HDA folder, home to all the custom tools used in this workflow.
 These HDAs are already referenced in the main file, but they can also be reused in any Houdini project.
Now let’s jump into Houdini where I have this scene loaded.
 Here everything is organized into three main parts, each containing its own network and nodes.
 First is the source network.
 This is the full resolution layout, using dense, high fidelity geometry.
In the sample file, it’s a simple procedural setup.
 But for this demo, I'll be using the moon environment as the example.
Next is optimized, where the scene is rebuilt for real-time performance using the techniques discussed in this video.
And finally, everything is assembled into USD, where it can be exported to a runtime editor like Reality Composer or Unity.
Now let's look at the first custom HDA, a utility that will be useful for every step of this workflow.
 This is the Boundary Camera HDA, built to visualize how immersive content will be experienced.
This tool provides a frame of reference to define the edges of the Immersive Boundary.
 It has multiple camera setups, making it easy for you to see your content from actual points of view.
Adjustable parameters are available to transform the camera easily, allowing you to track the scene as you optimize the content.
With the project set up, let’s dive into the first set of tools for optimizing geometry.
 As mentioned earlier, this scene has a massive amount of polygons, hundreds of rocks, and kilometers of terrain in all directions.
 So how can you go about reducing the complexity of something this massive? Well, you might use a Level of Detail system to simplify individual assets.
 And for some types of content, this is the right solution.
 But let's go beyond the usual methods and leverage the Immersive Boundary to its full potential.
 I’ll utilize three techniques to simplify the same, reducing the total amount of polygons, starting with adaptive polygon reduction.
 Instead of reducing assets one by one, let’s optimize for every triangle, and base the reduction only from the possible viewpoints.
Here, triangle size is visualized where red indicates high density and blue means larger, more simplified areas.
 In this setup, triangles are preserved to retain complexity only where needed, and the density organically scales up and down with detail fading out gradually, rather than dropping off all at once.
This ensures that important details stay sharp, avoiding faceted low-poly artifacts that can quickly break immersion.
 That’s the concept.
 Now let’s look into how to build it.
This is where custom made tools come into play, built to calculate and automate the process.
 By using attribute-driven parameters, you can guide what gets preserved and what gets reduced, entirely through procedural logic.
It’s built on Houdini’s native PolyReduce tool, set up to handle multiple viewpoints.
 In this visualization, the green sphere marks a potential position used to measure silhouette importance.
The HDA is driven by a set of sample points distributed along the Immersive Boundary.
 This ensures the geometry holds up from every angle the viewer can experience.
Let’s take a look at where this fits into the pipeline.
 After importing the high-poly source asset, the first step is to split the content, making it easier to tackle each type independently.
Let’s jump into Houdini and walk through an example for setting up one of these HDAs.
In the sample file, all rocks are reduced in a single pass, using just one Adaptive Reduce HDA.
 But this is a pretty heavy operation, because the amount of triangles on the source rocks alone is over 22 million.
In order to iterate quickly, I’ve isolated a small section of the rocks so I can more easily demonstrate how just one of these HDAs is set up.
Starting with these rocks, I’ll drop down a new instance of the Adaptive Reduce HDA and wire it in.
Then I’ll switch over to the immersive camera so I’m working from the perspective inside the Immersive Boundary.
Now if I enable and cook the HDA, a heat map is overlaid.
 This gives immediate feedback on where the density is across the surface after reduction.
 The total triangle count is controlled by this parameter up top.
And you can adjust this to get your desired output.
As mentioned earlier, many of these tools rely on sample points to evaluate geometry from multiple viewing angles.
 There is a dedicated HDA for this called Boundary Samples, which generates points inside the Immersive Boundary.
I'll create this new HDA and assign it to the Adaptive Reduce HDA.
Once I assign these points, the tool will reduce based on these positions.
 More triangles are now retained along silhouette edges, with the total amount of triangles unchanged.
 This is driven by the silhouette parameter.
By adjusting this value, I can spend more or less triangles on these features.
 Now for the rocks further away.
 I’ll adjust the distance weighting so I retain less geometry, but still retain enough for the silhouette.
That’s the basic setup for many of these tools.
 Create an HDA, assign sample points, and adjust its parameters to Dial in the results.
By using this tool once for the terrain and again for all the rocks, the geometry is optimized for everything within the first kilometer.
For the terrain, it’s reduced more aggressively based on distance, gradually reducing the density of triangles out towards the horizon, while the rocks are simplified to retain most of the silhouette, reducing everything else.
 This optimizes for the first kilometer.
 But what about the remaining vista? For distant objects, you can use a technique often used in game development.
 Let’s utilize Billboards.
 There’s a natural limit for how much depth in parallax that can be perceived at long range.
 This means you don’t need full 3D geometry for far-off objects.
 For the Immersive Boundary, depth cues start to flatten out from 1 to 3 kilometers.
 So you can reduce these areas to a flat image without much perceivable loss of depth.
This technique takes complex 3D objects and renders them to a flat geometry, oriented to the Immersive Boundary.
 For large environments, it can be set up as a panoramic strip that transitions seamlessly with the rest of the scene.
 For the Moon, I’ve chosen 1 kilometer so I can save even more triangles, allowing me to put even more details in the nearby assets.
Billboard techniques often utilize transparency in order to keep the geometry simple, but this tool builds the silhouette using the actual geometry.
 That means the border matches the original source, vertex for vertex, with no need for transparent materials.
 This can be used for all sorts of objects, from hard surface rocks to organic foliage.
 It works by ray casting each vertex towards a fixed position.
 Then those rays collide with a simple shape, like a sphere or a cylinder, positioned at a predefined distance.
At first, the geometry is a mess.
 But once the points are re-triangulated, you get a clean, simplified silhouette that still holds up from the viewer’s perspective.
The Vista Billboard HDA handles this entire setup procedurally.
 It’s placed right after the train is imported from the source layout.
With this tool, geometry beyond 1 kilometer drops from several million polygons to just a few thousand.
 This simplified mesh is combined with earlier optimizations, giving a fully optimized view of the environment.
Let's check in on the savings so far.
 With adaptive mesh reduction, and by converting the distant geometry into Billboards, the triangle count is already down to just 350,000.
 That’s a massive drop from the original 100 million polygon layout.
 The environment is optimized so that everything visible is much more efficient.
 The next step is optimizing for what’s not visible, using Occlusion Culling.
Occlusion Culling eliminates geometry that’s completely hidden, saving resources by not rendering or texturing something the viewer will never see.
 Some Culling happens at runtime, which can remove entire meshes when they’re not visible.
 But let’s make this even easier to render, and remove every Occluded triangle directly from the geometry.
 This next HDA removes triangles by ray casting millions of points in all directions, testing which polygons are visible.
 If even one ray hits any part of a triangle, it’s marked to keep, but everything else is removed, visualized here in red.
This toolset includes two types of Culling: Backface Removal and Occlusion Culling.
 Both run in tandem after polygon reduction.
First, Remove Backfaces.
 It uses a dot product comparison to find polygons that always face away from the Immersive Boundary.
 It’s fast, reliable, and a great first pass.
 Culled polygons show up in wireframe, and the triangle savings appear right below each HDA.
In the moon’s case, just under 60,000 are removed.
Now for the second tool, Occlusion Culling.
 It works by ray casting from each sample position to test visibility.
 Accuracy depends on how many sample points you use and how many rays are cast from each one.
 In this example, 110,000 triangles were Culled.
This is in addition to what was already saved by Backface Culling.
Together, these tools form a powerful combo.
 About 50% of the remaining triangles are removed in this pass, visualized here in blue, showing the updated borders of the geometry.
 And from the actual vantage points, it’s like nothing happened at all.
Let's check in on the final numbers.
The triangle count is now just 180,000.
 For an environment with this level of complexity, that's a fantastic place to be.
Here’s a side-by-side comparison, using a depth map for visualization.
 Same silhouettes, almost identical, but now it’s running on just a fraction of the original geometry.
The real utility of building your workflow this way isn’t just in the numbers.
 It’s in the ability to fine-tune every step of the way.
 Dial it up for higher quality, Dial it down for more performance.
 With a procedural tool chain, you are in full control of the process.
 Now that the geometry is optimized, it’s time to do the same for UV layout and textures.
It may not look like it at first glance, but even a barren landscape like this is packed with unique detail.
 Dozens of gigabytes of high-fidelity PBR maps rendered with past-traced lighting that spans every surface.
 That means every pixel from every angle is unique.
 All that data is a serious challenge.
So how can all that detail be preserved on the optimized geometry? It starts with UVs, the 2D coordinate system that controls how textures map to geometry, and once again leveraging the Immersive Boundary to rethink how texture data is organized.
Let’s start right at the edge of the Immersive Boundary, isolating just the first 5 meters.
 Assets in this space can be viewed from almost any angle, So the texel density needs to stay consistent across every surface.
 That's what this grid pattern shows: area-based UV mapping, so every surface holds up under close inspection.
Now, outside the Boundary, things get interesting.
 Assets are only seen from limited angles and distances, so the usual surface area mapping falls short, wasting texels on areas the viewer can never get close to.
 So the strategy needs to shift.
Instead of area-based UVs, projection-based mapping takes over, aligning textures to screen area within the Immersive Boundary.
This is screen space mapping, where texel density scales naturally based on how the viewer actually sees it.
 Distant surfaces get fewer texels, while closer, more visible areas are allocated more resolution.
 To utilize screen space mapping, this next tool uses a technique called spherical projection.
Think of it like wrapping a sphere around the viewer, then projecting the environment onto that surface.
 To make this work for textures, both UVs and pre-rendered images need to speak the same language.
So when rendering the source environment, the same spherical projection is used for the camera.
This means the image lines up perfectly with the UV layout.
Let’s try and apply this setup to the optimized geometry.
First impressions, looking great.
 With the wireframe enabled, geometry and the rendered image stitched together perfectly.
 But what happens if you stand up and look around a little? Uh-oh, something doesn't look right.
Actually, entire parts of the landscape are missing from the texture.
 Surfaces are grabbing whatever happens to be in front of them, smearing small rocks across big rocks and big rocks across mountains.
For an Immersive experience like this, more information is needed to cover the scene from every angle.
 But there are several challenges.
 First, UV overlapping, visualized here in red.
 At glancing angles, surfaces compress in UV space, causing triangles to fold on top of each other and stack on top of the geometry behind it.
This means even if a render has those surfaces, the data has nowhere to be stored.
 The next issue is texel scaling.
 It’s only accurate from the actual projection position.
 As soon as you move, texel density starts to break down.
And third, the render itself.
 A single panoramic capture can’t see everything.
 Some surfaces either lack in detail or are missed completely.
 To fix these issues, you'll need to project UVs from multiple angles, finding the optimal position for each surface, then scale texel density from every viewpoint, and untangling overlapping UVs along the way.
 It's a lot.
But when you approach it piece by piece, you can spread the problem across multiple easier to manage steps.
 You don't have to reduce the complexity, you just need the right system to take control of it.
So first, let’s split off the geometry inside the Immersive Boundary, and let’s focus on the rest of the environment.
This technique happens in two passes.
 First, by splitting the mesh into smaller sections so the geometry is not limited to a single UV projection.
 It’s kind of like pre-defining your UV islands, defining the borders before any UVs are actually created.
 Next, each section gets its own UV projection from a custom position that maximizes visibility.
For the moon, let’s focus on the rocks again, as they’re the most challenging.
 This is because not all sides are visible from a single viewpoint.
 To help with that is the Mesh Partition HDA.
 It tries to minimally split the mesh into as few islands as possible so that each can be seen clearly from at least one position.
For example, on this hero rock, faces are split so UVs can be projected from one side then the other, giving the maximum level of resolution needed for both.
The Multi-Partition HDA fits into this workflow right after the geometry optimization.
 Once the partitions are in place, they can all be processed together using the Multi-Projection HDA.
This tool runs through each partition created by the previous HDA, projecting UVs from the point where that piece appears largest in screen space.
It draws a line back to the origin of each projection, showing exactly where the UVs were created from.
It might look a little chaotic at first, but once it runs, the heavy lifting is already done.
Let's take a look at the results.
All the UV islands are stacked on top of each other, because each one was aligned to the center of its projection.
 Now they just need to be laid out and packed into a final UV atlas.
That's the full setup.
 With a pass each for the rocks and the terrain, first the mesh was split procedurally.
 Then new UVs are projected onto each section.
 The result is more accurate texel scaling, with minimal overlap and distortion.
With this setup, the entire environment can fit on just two textures: one for the Immersive Boundary scaled by surface area, and one for everything beyond it.
 Thanks to screen-space scaling, both can be similar texture sizes.
 That's a massive compression.
 Kilometers of terrain condensed into the same footprint as just the first few meters.
With the optimization complete, the moon environment is now ready to bake pre-rendered images.
Rendering and baking are deep topics on their own, so here the focus will be on just a few key concepts that you’ll need to bake it all down.
 In Houdini, baking can be done using spherical renders.
 Here's how it works.
 A temporary spherical projection maps the render onto the geometry.
 That projection is then baked into the final UV layout.
 But keep in mind, this only captures one point of view.
 To fill in the missing data, one option is to use a projection method that isn’t view dependent.
If you’ve worked in games, you’ve probably done this before, baking high poly detail onto a low-poly mesh.
 It’s the same idea here, projecting directly from the surface of the source geometry onto the optimized UV atlas.
 This guarantees full coverage across every surface.
By compositing these techniques together and applying them as an unlit material on the optimized geometry, you end up with a fully textured environment.
 From dozens of gigabytes of high resolution textures, Now even massive environments can be compressed into less than a couple hundred megabytes, while still preserving the high-end visuals.
Everything is now ready to be exported for Vision Pro.
 The last step is setting up the USD.
 But a well-designed USD doesn’t just keep things tidy.
 It unlocks a final optimization that can push performance even further.
After optimization, the environment now exists as a unique mesh.
 If you export this way, every triangle will get sent to the GPU at runtime, even if sections are completely outside your view.
 To mitigate this cost, it needs to be set up for optimal Frustum Culling.
 The USD hierarchy can be used to partition the scene, allowing the renderer to automatically cull geometry based on the bounding box of each entity, improving performance by removing objects not visible in the camera’s field of view.
Here's a visualization of how this works.
 The blue wireframe highlights entities being unloaded from the GPU, as the viewer looks around and moves through the scene.
This workflow uses two types of partitioning, depending on whether the geometry is inside or outside the Immersive Boundary.
 We have a custom HDA made for each, both generating attributes to define each new partition, laying the groundwork for the USD layout.
 Inside the Boundary, geometry is split using the Boundary Partition HDA, optimal for removing dense geometry underfoot.
 For everything outside the Boundary, the Frustum Partition HDA takes over, dividing the mesh into progressively larger tiles, optimized to make each a relatively similar size and screen space.
All the optimizations so far have been at the surface level, modifying a data table of points and primitives instead of true objects.
 But USD requires a hierarchical structure, organized into transforms, mesh primitives, and geometry subsets.
 Before export, the geometry needs to be cleaned up and structured into something USD can interpret.
 For this setup, only a few attributes need to be carried forward.
 A name attribute becomes the primitive name in the USD hierarchy, and group assignments define the subsets used for Frustrum Culling.
 The two partition HDAs have already created these groups, so they just need to be passed to the USD.
 With those attributes set, the rest is straightforward.
 Import the geometry into Solaris, then just enable the Subset Groups and declare your custom group names.
 Your partitions are now geometry subsets, ready for Frustrum Culling in real-time.
The Moon environment started with dense geometry, massive textures, and complex shading.
 And now this optimization is complete.
 What began as a cinematic render is now real-time ready, powered by a procedural toolset for immersive content.
 Let’s wrap it all up by looking at the real impact, how these optimizations enable building a wide range of Immersive experiences.
For the Moon, the scene started at over 100 million triangles.
 That was reduced all the way down to under 200,000, with less than 100,000 visible on screen at any time, thanks to Frustum Culling.
 It’s possible to push that number even higher, especially for a static scene.
 But there are other costs to consider, such as complex shading, animated characters, and interactive elements.
 So keeping triangle counts low helps reserve budget for the content that matters most.
 On the texture side, spherical projections have made a huge impact.
 They reduced the entire environment to under 250 megabytes of texture memory.
 That number will vary for your projects but can remain relatively consistent because most surfaces are scaled to screen space.
 As a bonus, you can use as many unique textures on the source input as needed, since it all gets baked down to the same texture atlas in the end.
On the object side, the total entity count is kept under 200 assets, and in any given frame there are typically fewer than 100 draw calls.
 That’s possible because all assets are merged and baked into consistent mesh partitions, regardless of how many unique objects you’re seeing started with.
The tools used in this video are designed to be as robust as possible, so you can repurpose them across a variety of possibilities.
If you are building a portal experience, you can use a very similar set of tools with adjusted parameters.
 Here, modifying the Occlusion Culling HDA will remove triangles not seen through the extents of a portal.
For some environments, multiple Immersive Boundaries might be the goal.
Instead of rebuilding the scene for each position, you could use multiple sets of sample points to optimize for each Immersive experience.
With just a small change, most of the tools should work the same, scaling triangles for both Boundaries, and projecting UVs so each has the minimal texel density needed, allowing to share one mesh and one set of textures for both vantage points.
 Every environment you work on is going to be different.
 There is no one-size-fits-all workflow.
 But if you assemble a collection of tools in your belt, you can choose the best approach for each project.
 For example, rocky environments like the moon or Joshua Tree, atmospheric scenes such as Mount Hood or Haleakala, or even hard surface interiors like a conference room or a theater.
 Experiences like these can be built using the techniques discussed here and more.
 Some scenes demand heavier shader complexity, requiring geometry to be scaled back to maintain real-time performance.
Others, like a conference room, are hand modeled for efficiency from the start, so heavy optimizations would be overkill.
 To dive deeper into the technical setup, you can start with the included sample project.
 It contains the same workflow and tools from this video, and is free to use and modify.
 The README file contains breakdowns for every tool, plus a few extra HDAs I didn't cover.
 For more platform-specific guidance on optimization, check out “Optimize your 3D assets for spatial computing.
” The main takeaway is high-fidelity content doesn't have to be expensive.
In the early days, developers couldn’t render every detail.
 They had to build creative techniques to do more with less.
 Those same principles can empower your workflow today.
It’s about optimizing with intention, right alongside the creative process.
 With great tools, you can push the boundaries of what’s possible, building environments that deliver deeper Immersion and a strong sense of presence.
 Thank you.
