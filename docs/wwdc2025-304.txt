Hi, I’m Dave, an engineer on the visionOS Spatial Media team.
 Today, we’re going to explore all of the different types of video media available on visionOS, including some new ones in visionOS 26.
 Vision Pro is a spatial computer that shows high quality pass-through video of the surrounding world.
 Because Vision Pro is with you wherever you look, content creators have an entire 360° around the viewer in which they can present engaging content.
 They’re not just limited to a flat screen in front of the viewer.
 This enables a wide range of media playback options, some of which are only possible on a spatial computer.
 Let's take a look.
 We’ll start by exploring how 2D and 3D video can be presented on visionOS, including some cool new features coming in visionOS 26.
 We'll see how spatial video enables people to capture their own stereo videos, and some new ways you can play those videos in your app.
We’ll learn about new immersive video formats for 180°, 360°, and wide FOV video.
And for the ultimate immersive experience, there's Apple Immersive Video.
 We’ll see how you can now create your own Apple Immersive content and tooling.
 Finally, we’ll explore how all these different video profiles compare to help you choose the right one for your app.
 Let's get started with 2D and 3D video.
 Vision Pro is a great device for enjoying 2D movies and TV shows, and as a stereo device, it’s perfect for watching stereoscopic 3D movies too.
 2D videos can be played inline anywhere in your app’s UI using an embedded playback experience, where the video appears alongside other UI elements.
Here’s an example of playing video inline in freeform as part of a board full of content.
Note that if you embed a 3D video inline, it gracefully falls back to playing in 2D.
And both 2D and 3D video can be played in an expanded experience on a floating screen in the shared space.
 Note that an expanded experience is required to play a 3D movie stereoscopically.
2D and 3D video can also transition to a docked state in a virtual environment.
Here’s an example from the Destination Video sample code project, available from developer.
apple.
com, where the video fades out from its expanded view and fades back in, docked within the app’s own custom studio environment, created with Reality Composer Pro.
 The video automatically shows dynamic light spill to make it feel like an integral part of that environment.
Docking in an environment is a great example of how spatial computing enables video playback to move beyond the bounds of a fixed single screen.
 And there are even more ways that 2D and 3D video playback can take advantage of Vision Pro’s infinite canvas.
visionOS 2 introduced multi-view video, where viewers can enjoy multiple camera angles of a single event or multiple sources of video simultaneously in Vision Pro.
A new in visionOS 26, 2D and 3D videos can specify a per-frame dynamic mask to change or animate their frame size and aspect ratio, to accentuate a story point, or to combine archival and modern-day footage in a single scene, without needing to show black bars for letterboxing or pillarboxing.
These kinds of seamless transitions, with a framing that best suits each shot, are only possible on a spatial computer.
For more information, see the “Rectangular Mask Payload Metadata” document on developer.
apple.
com.
3D video is the medium of choice for professional stereo productions designed to be viewed on a big screen.
 But stereo video doesn’t have to require a movie studio.
 Vision Pro supports a new kind of stereo media that people can capture themselves, known as Spatial Video.
 People can shoot comfortable, compelling stereo content on devices like iPhone without needing to be an expert in the rules of 3D filmmaking.
Spatial video is stereo video with additional metadata, that enables windowed and immersive treatments on Vision Pro to mitigate common causes of stereo discomfort.
By default, spatial video renders through a window, with a faint glow around the edges.
 It can also expand into an immersive presentation, where the content is scaled to match the real size of objects.
 The edge of the frame disappears, and the content blends seamlessly into the real world.
Spatial videos automatically fall back to 2D presentation on other platforms, enabling them to be played on all Apple devices.
 This enables people to capture spatial content to enjoy on Vision Pro, and still share their memories in 2D with friends and family who don’t yet have a Vision Pro.
We saw earlier how 2D and 3D video is presented on a flat screen.
 For spatial videos, we inset that flat screen behind a window and blur the edges of the window to soften them.
 This helps avoid issues when an object is clipped by the window more in one eye than the other, which can be uncomfortable to view.
Spatial videos can be captured today on iPhone 15 Pro, iPhone 16, and iPhone 16 Pro, in the Camera app and in your own app via AVCaptureDevice APIs.
 Spatial videos can also be captured on Apple Vision Pro and, with Canon’s R7 and R50 cameras with a Canon DUAL lens.
In visionOS 2, spatial videos can be played with spatial styling in your own app with the QuickLook PreviewApplication API.
 In visionOS 26, we’re bringing that same spatial styling to all of Apple’s media playback frameworks.
 We’re adding QLPreviewController support in QuickLook, plus support in AVKit, RealityKit, Safari, and WebKit, enabling you to incorporate spatial videos however you choose in your app, with support for HTTP Live Streaming, or HLS.
And if you’re looking to edit and combine spatial videos in your app to create a longer narrative, the format is now supported in industry standard editing tools, such as Compressor, DaVinci Resolve Studio, and Final Cut Pro.
 To see more examples of what’s possible with spatial videos and spatial photos as well, check out the Spatial Gallery app on Vision Pro.
 To learn more about the spatial photo and video formats, check out “Build compelling spatial photo and video experiences” from WWDC24.
 And for more about the new ways you can play spatial videos in your app, check out “Support immersive video playback in visionOS apps”.
 And talking of spatial photos, there’s new RealityKit API in visionOS 26 for displaying spatial photos in your app and for converting 2D photos to a 3D spatial scene.
To learn how to present spatial photos and generate spatial scenes using the new ImagePresentationComponent and Spatial3DImage APIs, check out “What’s new in RealityKit”.
We saw earlier how 2D, 3D, and spatial videos are all played on a flat surface in Vision Pro.
 This is because these videos typically use what's known as a rectilinear projection, as seen in this photo of the Apple Park Visitor center.
 Rectilinear just means that straight lines are straight.
 There’s no lens curvature or warping in the video.
 And because of this, these kinds of videos feel correct when viewed on a flat surface that also doesn’t have any curvature or warping.
We saw how the playback experience for these rectilinear videos on Vision Pro expands that flat surface to fill more of the viewer’s field of view, via docking for 2D and 3D video, and immersive presentation for spatial video.
But on a spatial computing device, we’re not just restricted to a flat surface in front of the viewer.
 There’s even more space around them we can fill with pixels.
 And there are other non-rectilinear video types that are great for filling even more of the viewer’s field of view, by presenting video on a curved surface, not a flat one.
 visionOS 26 adds native support for three of these non-rectilinear media types: 180° video, 360° video, and wide FOV video.
 Let's take a look.
 180° video is presented on a half sphere directly in front of the viewer.
 Video is projected onto that half sphere to completely fill the viewer’s forward field of view.
 In this example, it’s a 180° video of the pond at Apple Park.
From the viewer’s point of view, it’s like being there.
 This is a great way for content creators to transport their viewers to amazing locations.
 180° video is typically captured in stereo.
 And because it’s stereo, and completely fills your forward field of view, it feels like you’re looking at the real thing.
360° video takes things a step further, filling the entire world with content.
 With 360° video, the content literally surrounds the viewer, giving them the freedom to look wherever they like.
 Here’s an example of a 360° video captured underneath the rainbow at Apple Park.
 The viewer can look around at any angle and feel like they are right there beneath the rainbow.
 Everything looks just as it would if they were there in person.
To achieve this, the 360° video is wrapped onto a sphere completely surrounding the viewer, centred on their eyes and filling their field of view whichever way they look.
A rectangular video frame, twice as wide as it is high, is used to achieve this, covering 360° of the scene across its width and 180° across its height.
 This video is mapped onto a sphere around the viewer with an equirectangular projection.
 This is like how a map of the world is drawn, where the north and south poles are stretched horizontally, to show a flat representation of a sphere.
180° video’s projection is similar to 360° video, but for half a sphere.
 This is known as a half-equirectangular projection.
 Half-equirectangular videos are square and map their square video frame onto the half sphere in the same way 360° videos do for a full sphere.
For stereo 180° video we simply have two squares of video, one for each eye.
Many existing stereo 180° videos encode these two squares side-by-side in a single pixel buffer that’s twice as wide as the resolution per eye.
 This is known as side-by-side or frame-packed encoding.
 However, there’s a lot of redundancy here.
 Because they’re two views of the same scene, the left and right eye images are very similar.
 Vision Pro takes advantage of this similarity to use a different approach for encoding stereo video, known as multiview encoding.
Apple platforms already have hardware support for HEVC, or High Efficiency Video Coding, a fast modern codec for video compression.
And so for stereo videos, we use MV-HEVC, or MultiView HEVC.
For stereo video, MV-HEVC encodes each eye into its own pixel buffer and writes those two buffers together in a single video track.
 It takes advantage of the image similarity to compress one eye’s pixels relative to the other, encoding only the differences for the second eye.
 This gives a smaller encoded size for each frame, making stereo MV-HEVC videos smaller and more efficient.
 This is particularly important when streaming stereo video.
To learn more about multiview encoding and MV-HEVC, check out “Deliver video content for spatial experiences” from WWDC23.
Now, there’s a third kind of non-rectilinear video that’s new in visionOS 26.
 Wide FOV video from action cameras such as the GoPro HERO13 and Insta360 Ace Pro 2.
These action cameras capture highly stabilized footage of whatever adventures you take them on.
 They capture a wide horizontal field of view, typically between 120° and 180°, and often use fisheye-like lenses that show visible curvature of straight lines in the real world.
 This enables them to capture as much of the view as possible.
Traditionally, these kinds of videos have been enjoyed on flat-screen devices like iPhone and iPad, and this is a fun way to relive the adventure.
 But in visionOS 26, we’re introducing a new form of immersive playback for these kinds of action cameras, recreating the unique wide-angle lens profile of each camera as a curved surface in 3D space, and placing the viewer at the center of the action.
Because that curved surface matches the camera’s lens profile, it effectively undoes the fisheye effect, and the viewer sees straight lines as straight, even at the edge of the image.
 This recreates the feeling of the real world as captured by the wide-angle lens, while still displaying a maximum field of view.
Wide FOV action cameras have lots of different modes, and the right mode to use depends on what you're shooting.
Some modes prioritize the middle of the lens, for cases where that’s where the action is.
Other modes prioritize the edges of the frame to capture as wide a field of view as possible.
 And there are many more permutations.
 To represent all of these different modes and lens configurations, we need a more expressive way to describe how the real world gets mapped to pixels in an image.
 For this, we use math.
Different lenses have different shapes and profiles.
To model these different lens profiles, we define a bunch of parameters for things like the focal length, skew, and distortion of the lens.
 Camera and lens manufacturers can tailor these parameters to describe a wide variety of lenses, and how those lenses map the real world onto pixels in an image.
Because it's defined by parameters, we call this projection a parametric immersive projection.
In visionOS 26, these immersive video types, 180°, 360°, and wide FOV video, are supported natively on visionOS via a new QuickTime movie profile called Apple Projected Media Profile, or APMP.
 If you’ve already worked with QuickTime movies and the spatial video format, this profile will feel very familiar, with new fields added to describe each of the new projection types.
We’ve updated the Spatial and Immersive Media Format Edition spec on developer.
apple.
com, to cover all of the details of this new profile.
 We’ve also added automatic conversion to APMP for many existing videos, both in files on Vision Pro and in your own app.
Stereo 180° videos captured with Canon’s EOS VR system and converted with the EOS VR Utility, are automatically converted to 180° APMP when opened on Vision Pro.
360° videos from devices like the GoPro MAX and Insta360 X5 are automatically converted to 360° APMP.
180° and 360° videos that conform to the equirectangular version of Google Spherical Video v1 or v2 are also detected and converted.
And straight-off-the-camera videos from recent action cams like the GoPro HERO13 and Insta360 Ace Pro 2, are automatically converted to wide FOV APMP.
The same is true for single-lens captures from the 360° cameras mentioned earlier.
 We’ve also updated the avconvert command-line tool on macOS to convert existing 180° and 360° content to APMP on Mac.
Just like spatial videos, APMP can be played on visionOS 26 by all of Apple’s media playback frameworks, with support for HTTP Live Streaming.
Note that APMP content is supported for expanded and immersive playback, but not for embedded inline playback.
Playing video immersively puts the viewer’s head right where the camera was during capture, even if that camera was strapped to the end of a surfboard.
 This means that immersive playback is especially sensitive to camera motion.
 To help mitigate this, we’ve added automatic high motion detection in QuickLook, AVKit, and RealityKit.
 Playback will automatically reduce immersion when high motion is detected, which can be more comfortable for the viewer during high motion scenes.
 There are options in the Settings app to customize high motion detection to match the viewer’s personal level of motion sensitivity.
To dive even deeper into the new Apple Projected Media Profile, including how to read and write it for your own content, check out “Learn about the Apple Projected Media Profile”.
Now, for the ultimate immersive experience, there’s Apple Immersive Video, which we’re making available to developers and content creators for the first time this year.
 Here’s an example from the Apple TV+ series, “Wild Life”, which transports viewers to meet the elephants at Kenya’s Sheldrick Wildlife Trust.
 This scene would be almost impossible to experience in reality, but with Apple Immersive Video, it feels like you're truly there.
If you’re working with the Blackmagic URSA Cine Immersive camera, you can create, edit and distribute Apple Immersive Video yourself.
The specs for the URSA Cine Immersive camera are astounding.
 Every lens in every camera is individually calibrated at the factory, using the parametric approach we saw earlier, but tuned to that individual lens.
The Cine Immersive captures stereo video with 8160 x 7200 pixels per eye.
 That’s 59 megapixels per eye, at 90 frames per second.
That’s over 10 billion pixels per second.
The URSA Cine Immersive captures up to 210° FOV horizontally, and 180° vertically, with a sharpness approaching that of the human eye.
In visionOS 26, your app can play Apple Immersive Video with all the same media frameworks as APMP, with support for HTTP Live Streaming.
 Like APMP, Apple Immersive Content is supported for expanded and immersive playback, but not for embedded inline playback.
 So how do creators bring their Apple Immersive content to Vision Pro? There are four main steps in the content creation pipeline.
First, capture video on the URSA Cine immersive camera.
 Next, edit that video in DaVinci Resolve Studio.
Then preview and validate content using the new Apple Immersive Video Utility apps for macOS and visionOS.
 And finally, segment content in Compressor for distribution via HTTP Live Streaming.
For pro app developers who want to create their own tools to work with Apple Immersive Video, such as a non-linear editor, or a custom pipeline tool, we’re introducing the ImmersiveMediaSupport framework for macOS and visionOS 26.
 This framework enables you to read and write Apple Immersive content programmatically.
And there are many more capabilities of Apple Immersive Video that are unique to the format, including per-shot edge blends, custom backdrop environments, the new Apple Spatial Audio Format, and live preview on Apple Vision Pro.
Let’s dig a little deeper into just one of these, per-shot edge blends.
Every shot in an Apple Immersive Video can define a custom edge blend curve that best suits its content and framing.
This isn’t a baked-in mask.
 It’s a dynamic alpha blend curve that feathers the edges of the shot to transition it into a custom backdrop environment.
And that’s just a flavor of what’s possible with Apple Immersive Video.
 Check out “Learn about Apple Immersive Video technologies” for more information.
 Let’s finish by taking a look at all of these media profiles together, to help decide which ones are right for your app’s experience.
 To recap, we have 2D and 3D video presented on a flat screen, spatial video, where stereo video is inset behind a flat screen for windowed presentation and shown at true scale when immersive, 180° and 360° video projected onto a half sphere, and a full sphere, wide FOV video with a curved mesh that matches the projection of a wide angle lens from an action cam, and Apple Immersive Video, with high resolution stereo video, perfectly calibrated to each lens in the camera that captured it.
Here are all the fundamentals for these different video profiles, including how they can be captured, their visual characteristics, their horizontal field of view, and their projection type.
 Note that 180°, 360°, and wide FOV videos can be mono or stereo.
 Typically 180° is stereo, and 360° and wide FOV are mono.
And here’s a reminder of how all of the different profiles can be presented during playback.
 Note that high motion detection is automatically enabled for all three APMP profiles.
 And 2D, 3D, and spatial videos all offer 2D playback when embedded in line, because of their rectilinear projection.
Vision Pro enables so many exciting video playback experiences, from new ways to experience 2D and 3D video, to new types of media that only make sense on a spatial computing platform.
 To get started with all these video profiles, check out our new immersive playback sample code projects for AVKit and RealityKit, and new sample code projects for writing and working with APMP and Apple Immersive Video.
 We’ve also provided example video downloads and HLS streams for spatial, 180°, 360°, wide FOV and Apple Immersive Video, available from developer.
apple.
com.
 For more information, check out related video sessions on immersive video playback, Apple Projected Media Profile, and Apple Immersive Video.
 And how to play all these media types on the spatial web in Safari and WebKit.
 And now: Go.
.
.
 immerse!